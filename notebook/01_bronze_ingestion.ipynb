{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64011e00-4e25-4891-9996-ac00fb16c8a9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import base64\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "CATALOG_NAME = \"spotify_etl\"\n",
    "BRONZE_SCHEMA = \"bronze\"\n",
    "\n",
    "today = datetime.utcnow()\n",
    "print(today)\n",
    "yesterday = today - timedelta(days=1)\n",
    "print(yesterday)\n",
    "\n",
    "after_ts = int(yesterday.timestamp() *1000)\n",
    "print(after_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d2b6afa-c8bd-464e-a7e1-ec8212e23a9a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configs Uniti Catalog"
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. UNITY CATALOG CONFIGURATION ---\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{BRONZE_SCHEMA}\")\n",
    "\n",
    "print(f\"Using Catalog: {CATALOG_NAME}, Schema: {BRONZE_SCHEMA}\")\n",
    "\n",
    "# --- 2. CONFIGURE SECRETS AND API ---\n",
    "# Retrieve the secrets needed for refresh\n",
    "try:\n",
    "    # Pay attention to the scope and key names, make sure they match\n",
    "    CLIENT_ID = dbutils.secrets.get(scope=\"spotify_secrets\", key=\"user\") # You used 'user', usually it's 'client-id'\n",
    "    CLIENT_SECRET = dbutils.secrets.get(scope=\"spotify_secrets\", key=\"client_secret\")\n",
    "    REFRESH_TOKEN = dbutils.secrets.get(scope=\"spotify_secrets\", key=\"refresh-token\")\n",
    "except Exception as e:\n",
    "    print(\"ERROR: Could not read secrets (user, client_secret, refresh-token) from 'spotify_secrets'.\")\n",
    "    print(\"Make sure you followed the setup steps to create the secrets.\")\n",
    "    raise e\n",
    "\n",
    "# Spotify login URL\n",
    "TOKEN_URL = \"https://accounts.spotify.com/api/token\"\n",
    "BASE_URL = \"https://api.spotify.com/v1\"\n",
    "\n",
    "# Global variable to store the current token\n",
    "CURRENT_ACCESS_TOKEN = None\n",
    "\n",
    "print(\"Configuration and secrets have been uploaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "544733a2-51ca-4d6c-a0b6-41d7f54bcf37",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Token Management Logic"
    }
   },
   "outputs": [],
   "source": [
    "def refresh_access_token():\n",
    "    \"\"\"\n",
    "    Use REFRESH_TOKEN (from secrets) to get a new access_token.\n",
    "    \"\"\"\n",
    "    global CURRENT_ACCESS_TOKEN\n",
    "    print(\"Refreshing access token...\")\n",
    "    \n",
    "    auth_str = f\"{CLIENT_ID}:{CLIENT_SECRET}\"\n",
    "    auth_b64 = base64.b64encode(auth_str.encode()).decode()\n",
    "    \n",
    "    auth_data = {\n",
    "        \"grant_type\": \"refresh_token\",\n",
    "        \"refresh_token\": REFRESH_TOKEN\n",
    "    }\n",
    "\n",
    "    auth_headers = {\"Authorization\": f\"Basic {auth_b64}\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(TOKEN_URL, data=auth_data, headers=auth_headers)\n",
    "        response.raise_for_status()\n",
    "        token_data = response.json()\n",
    "        new_token = token_data.get('access_token')\n",
    "        \n",
    "        if not new_token:\n",
    "            raise Exception(\"Response did not contain an 'access_token'\")\n",
    "            \n",
    "        CURRENT_ACCESS_TOKEN = new_token\n",
    "        print(\"Access token successfully refreshed.\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR refreshing token: {e}\")\n",
    "        CURRENT_ACCESS_TOKEN = None\n",
    "        return False\n",
    "\n",
    "def make_api_call(url, params=None, retries=1):\n",
    "    \"\"\"\n",
    "    Wrapper function that handles API calls, token refresh, and retries.\n",
    "    \"\"\"\n",
    "    global CURRENT_ACCESS_TOKEN\n",
    "    \n",
    "    if not CURRENT_ACCESS_TOKEN:\n",
    "        if not refresh_access_token():\n",
    "            raise Exception(\"Could not obtain initial token.\")\n",
    "\n",
    "    headers = {\"Authorization\": f\"Bearer {CURRENT_ACCESS_TOKEN}\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        \n",
    "        # Check if the token has expired\n",
    "        if response.status_code in [401, 403] and retries > 0:\n",
    "            print(f\"Token expired ({response.status_code}). Refreshing...\")\n",
    "            if refresh_access_token():\n",
    "                print(\"Retrying API call with new token...\")\n",
    "                return make_api_call(url, params=params, retries=0)\n",
    "            else:\n",
    "                response.raise_for_status()\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    \n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"API error calling {url}: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96fbe302-12f9-4c0e-96b2-134198d95b6a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "add function"
    }
   },
   "outputs": [],
   "source": [
    "def chunk_list(data, size):\n",
    "    \"\"\"Splits a list into smaller chunks.\"\"\"\n",
    "    for i in range(0, len(data), size):\n",
    "        yield data[i:i + size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91835e0a-3202-4bb1-8541-56ab24e08753",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Funcții de Extragere (API -> Python)"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SPOTIFY BRONZE INGESTION - FETCH FUNCTIONS (OPTIMIZED)\n",
    "# ============================================================\n",
    "\n",
    "def get_play_history(unique_track_ids, unique_artist_ids):\n",
    "    \"\"\"Source A: Get recent listening history.\"\"\"\n",
    "    print(\"Fetching play history...\")\n",
    "    play_history_data = []\n",
    "    url = f\"{BASE_URL}/me/player/recently-played?limit=50&after={after_ts}\"\n",
    "    \n",
    "    try:\n",
    "        results = make_api_call(url)\n",
    "        for item in results.get('items', []):\n",
    "            if not item:\n",
    "                continue\n",
    "\n",
    "            track = item.get('track', {})\n",
    "            artist = (track.get('artists') or [{}])[0]\n",
    "\n",
    "            play_history_data.append({\n",
    "                'played_at': item.get('played_at'),\n",
    "                'track_id': track.get('id'),\n",
    "                'track_name': track.get('name'),\n",
    "                'artist_id': artist.get('id'),\n",
    "                'artist_name': artist.get('name'),\n",
    "                'album_id': track.get('album', {}).get('id'),\n",
    "                'album_name': track.get('album', {}).get('name'),\n",
    "                'context_type': (item.get('context') or {}).get('type'),\n",
    "                'duration_ms': track.get('duration_ms')\n",
    "            })\n",
    "\n",
    "            if track.get('id'):\n",
    "                unique_track_ids.add(track.get('id'))\n",
    "            if artist.get('id'):\n",
    "                unique_artist_ids.add(artist.get('id'))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error fetching play history: {e}\")\n",
    "    \n",
    "    print(f\"✅ Retrieved {len(play_history_data)} play history records.\")\n",
    "    return play_history_data\n",
    "\n",
    "\n",
    "def get_playlists_and_tracks(unique_track_ids, unique_artist_ids):\n",
    "    \"\"\"Source B: Get playlists and their tracks.\n",
    "    Ensures schema stability by emitting `followers` even if Spotify API doesn't return it on /me/playlists.\n",
    "    \"\"\"\n",
    "    print(\"Fetching playlists and their tracks...\")\n",
    "    playlists_data = []\n",
    "    playlist_tracks_data = []\n",
    "    \n",
    "    next_url = f\"{BASE_URL}/me/playlists?limit=50\"\n",
    "    \n",
    "    try:\n",
    "        while next_url:\n",
    "            playlists = make_api_call(next_url)\n",
    "            for playlist in playlists.get('items', []):\n",
    "                playlist_id = playlist.get('id')\n",
    "                if not playlist_id:\n",
    "                    continue\n",
    "\n",
    "                # Followers are NOT present on /me/playlists; keep column with None for schema stability\n",
    "                followers_total = None\n",
    "                followers_obj = playlist.get('followers')\n",
    "                if isinstance(followers_obj, dict):\n",
    "                    followers_total = followers_obj.get('total')\n",
    "\n",
    "                playlists_data.append({\n",
    "                    'playlist_id': playlist_id,\n",
    "                    'playlist_name': playlist.get('name'),\n",
    "                    'owner_name': (playlist.get('owner') or {}).get('display_name'),\n",
    "                    'followers': followers_total,  # stays in schema even if None\n",
    "                    'total_tracks': (playlist.get('tracks') or {}).get('total'),\n",
    "                    'description': playlist.get('description'),\n",
    "                    'snapshot_id': playlist.get('snapshot_id')\n",
    "                })\n",
    "\n",
    "                # Fetch all tracks from the playlist\n",
    "                tracks_url = f\"{BASE_URL}/playlists/{playlist_id}/tracks\"\n",
    "                while tracks_url:\n",
    "                    tracks = make_api_call(tracks_url)\n",
    "                    for item in tracks.get('items', []):\n",
    "                        track = item.get('track')\n",
    "                        if not track or not track.get('id'):\n",
    "                            continue\n",
    "\n",
    "                        artist_ids = [a.get('id') for a in (track.get('artists') or []) if a.get('id')]\n",
    "\n",
    "                        playlist_tracks_data.append({\n",
    "                            'playlist_id': playlist_id,\n",
    "                            'track_id': track.get('id'),\n",
    "                            'track_name': track.get('name'),\n",
    "                            'artist_ids': artist_ids,\n",
    "                            'album_id': (track.get('album') or {}).get('id'),\n",
    "                            'added_at': item.get('added_at'),\n",
    "                            'added_by': (item.get('added_by') or {}).get('id'),\n",
    "                            'duration_ms': track.get('duration_ms'),\n",
    "                            'popularity': track.get('popularity')\n",
    "                        })\n",
    "\n",
    "                        unique_track_ids.add(track.get('id'))\n",
    "                        for art_id in artist_ids:\n",
    "                            unique_artist_ids.add(art_id)\n",
    "\n",
    "                    tracks_url = tracks.get('next')\n",
    "\n",
    "            next_url = playlists.get('next')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error fetching playlists: {e}\")\n",
    "\n",
    "    print(f\"✅ Retrieved {len(playlists_data)} playlists and {len(playlist_tracks_data)} playlist-track mappings.\")\n",
    "    return playlists_data, playlist_tracks_data\n",
    "\n",
    "\n",
    "\n",
    "def get_full_metadata(unique_track_ids, unique_artist_ids):\n",
    "    \"\"\"Step 3: Data enrichment (tracks, artists). Emits `preview_url` consistently for schema stability.\"\"\"\n",
    "    print(f\"Fetching metadata for {len(unique_track_ids)} tracks and {len(unique_artist_ids)} artists...\")\n",
    "\n",
    "    tracks_data, artists_data = [], []\n",
    "\n",
    "    track_ids_list = list(filter(None, unique_track_ids))\n",
    "    artist_ids_list = list(filter(None, unique_artist_ids))\n",
    "\n",
    "    # 1️⃣ Fetch track metadata\n",
    "    try:\n",
    "        url = f\"{BASE_URL}/tracks\"\n",
    "        for batch in chunk_list(track_ids_list, 50):\n",
    "            params = {'ids': ','.join(batch)}\n",
    "            tracks_results = make_api_call(url, params=params)\n",
    "            for track in tracks_results.get('tracks', []):\n",
    "                if not track:\n",
    "                    continue\n",
    "                first_artist = (track.get('artists') or [{}])[0] or {}\n",
    "                album = track.get('album') or {}\n",
    "                tracks_data.append({\n",
    "                    'track_id': track.get('id'),\n",
    "                    'track_name': track.get('name'),\n",
    "                    'album_id': album.get('id'),\n",
    "                    'album_name': album.get('name'),\n",
    "                    'artist_id': first_artist.get('id'),\n",
    "                    'artist_name': first_artist.get('name'),\n",
    "                    'duration_ms': track.get('duration_ms'),\n",
    "                    'popularity': track.get('popularity'),\n",
    "                    'explicit': track.get('explicit'),\n",
    "                    'release_date': album.get('release_date'),\n",
    "                    'preview_url': track.get('preview_url')  # stays present even if None\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error fetching tracks metadata: {e}\")\n",
    "\n",
    "    # 2️⃣ Fetch artist metadata\n",
    "    try:\n",
    "        url = f\"{BASE_URL}/artists\"\n",
    "        for batch in chunk_list(artist_ids_list, 50):\n",
    "            params = {'ids': ','.join(batch)}\n",
    "            artists_results = make_api_call(url, params=params)\n",
    "            for artist in artists_results.get('artists', []):\n",
    "                if not artist:\n",
    "                    continue\n",
    "                followers_total = None\n",
    "                followers_obj = artist.get('followers')\n",
    "                if isinstance(followers_obj, dict):\n",
    "                    followers_total = followers_obj.get('total')\n",
    "                artists_data.append({\n",
    "                    'artist_id': artist.get('id'),\n",
    "                    'artist_name': artist.get('name'),\n",
    "                    'genres': artist.get('genres'),\n",
    "                    'followers': followers_total,\n",
    "                    'popularity': artist.get('popularity'),\n",
    "                    'uri': artist.get('uri')\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error fetching artists metadata: {e}\")\n",
    "\n",
    "    print(f\"✅ Retrieved: {len(tracks_data)} tracks, {len(artists_data)} artists.\")\n",
    "    return tracks_data, artists_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "077fcd47-76fe-4686-b0b6-4b0f20f565b9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Funcția de Încărcare (Python -> Delta)"
    }
   },
   "outputs": [],
   "source": [
    "def save_as_delta_table(all_data):\n",
    "    \"\"\"\n",
    "    Saves each list of dictionaries as a Delta table\n",
    "    in the 'bronze' schema of Unity Catalog.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- STARTING DATA LOAD (Saving to Delta Lake in {CATALOG_NAME}.{BRONZE_SCHEMA}) ---\")\n",
    "    \n",
    "    for table_name, data_list in all_data.items():\n",
    "        if not data_list:\n",
    "            print(f\"Skipping '{table_name}', no data found.\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            pd_df = pd.DataFrame(data_list)\n",
    "            spark_df = spark.createDataFrame(pd_df)\n",
    "            full_table_name = f\"{CATALOG_NAME}.{BRONZE_SCHEMA}.{table_name}\"\n",
    "            \n",
    "            print(f\"Writing {spark_df.count()} rows to {full_table_name}...\")\n",
    "            spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(full_table_name)\n",
    "            \n",
    "            print(f\"Successfully saved: {full_table_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving {table_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ce81742-7c14-4a33-bf72-ff6e3e061397",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Funcția Principală (Orchestrare)"
    }
   },
   "outputs": [],
   "source": [
    "print(\"--- STARTING SPOTIFY BRONZE PIPELINE ---\")\n",
    "\n",
    "unique_track_ids = set()\n",
    "unique_artist_ids = set()\n",
    "\n",
    "# --- 1. EXTRACT ---\n",
    "print(\"\\n--- STEP 1: EXTRACTION (API -> Python) ---\")\n",
    "play_history_table = get_play_history(unique_track_ids, unique_artist_ids)\n",
    "playlists_table, playlist_tracks_table = get_playlists_and_tracks(unique_track_ids, unique_artist_ids)\n",
    "\n",
    "# --- 2. ENRICH (API -> Python) ---\n",
    "print(\"\\n--- STEP 2: ENRICHMENT (API -> Python) ---\")\n",
    "tracks_table, artists_table = get_full_metadata(unique_track_ids, unique_artist_ids)\n",
    "\n",
    "print(\"\\n--- EXTRACTION SUMMARY ---\")\n",
    "print(f\"bronze_play_history data:   {len(play_history_table)} rows\")\n",
    "print(\"-----------------------------\")\n",
    "print(f\"bronze_playlists data:      {len(playlists_table)} rows\")\n",
    "print(f\"bronze_playlist_tracks data:{len(playlist_tracks_table)} rows\")\n",
    "print(\"-----------------------------\")\n",
    "print(f\"bronze_tracks data:         {len(tracks_table)} rows\")\n",
    "print(f\"bronze_artists data:        {len(artists_table)} rows\")\n",
    "\n",
    "\n",
    "# --- 3. LOAD (Python -> Delta) ---\n",
    "print(\"\\n--- STEP 3: LOAD (Python -> Delta) ---\")\n",
    "all_bronze_data = {\n",
    "    \"bronze_play_history\": play_history_table,\n",
    "    \"bronze_playlists\": playlists_table,\n",
    "    \"bronze_playlist_tracks\": playlist_tracks_table,\n",
    "    \"bronze_tracks\": tracks_table,\n",
    "    \"bronze_artists\": artists_table,\n",
    "}\n",
    "\n",
    "save_as_delta_table(all_bronze_data)\n",
    "\n",
    "print(\"\\n--- BRONZE PIPELINE FINISHED ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0aa5c18-b360-4c64-990b-cf49ba10430e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(unique_track_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "742a09e2-48c0-47a8-9b0a-417827738621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(unique_artist_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "852d7833-22c6-48e5-9f5e-eca899caad41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================\n",
    "# SCHEMA STABILIZATION + DELTA WRITE HELPERS (for Power BI)\n",
    "# Ensures columns like `followers` and `preview_url` always exist.\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql.types import (StructType, StructField, StringType, LongType, ArrayType, BooleanType)\n",
    "\n",
    "bronze_playlists_schema = StructType([\n",
    "    StructField(\"playlist_id\", StringType()),\n",
    "    StructField(\"playlist_name\", StringType()),\n",
    "    StructField(\"owner_name\", StringType()),\n",
    "    StructField(\"followers\", LongType()),         # keep for compatibility (may be NULL)\n",
    "    StructField(\"total_tracks\", LongType()),\n",
    "    StructField(\"description\", StringType()),\n",
    "    StructField(\"snapshot_id\", StringType())\n",
    "])\n",
    "\n",
    "bronze_tracks_schema = StructType([\n",
    "    StructField(\"track_id\", StringType()),\n",
    "    StructField(\"track_name\", StringType()),\n",
    "    StructField(\"album_id\", StringType()),\n",
    "    StructField(\"album_name\", StringType()),\n",
    "    StructField(\"artist_id\", StringType()),\n",
    "    StructField(\"artist_name\", StringType()),\n",
    "    StructField(\"duration_ms\", LongType()),\n",
    "    StructField(\"popularity\", LongType()),\n",
    "    StructField(\"explicit\", BooleanType()),\n",
    "    StructField(\"release_date\", StringType()),\n",
    "    StructField(\"preview_url\", StringType())      # keep for compatibility (may be NULL)\n",
    "])\n",
    "\n",
    "def write_delta_with_schema(df_list_or_dict, table_full_name, schema):\n",
    "    \"\"\"\n",
    "    Create a DataFrame with an explicit schema and write it to Delta with overwriteSchema=true.\n",
    "    df_list_or_dict: list[dict] or pyspark.sql.DataFrame\n",
    "    \"\"\"\n",
    "    if isinstance(df_list_or_dict, list):\n",
    "        df = spark.createDataFrame(df_list_or_dict, schema)\n",
    "    else:\n",
    "        df = df_list_or_dict  # assume already a DataFrame\n",
    "    (df.write\n",
    "       .format(\"delta\")\n",
    "       .mode(\"overwrite\")\n",
    "       .option(\"overwriteSchema\", \"true\")\n",
    "       .saveAsTable(table_full_name))\n",
    "    print(f\"✅ Wrote {table_full_name} rows={df.count()} (schema enforced).\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6722996605405841,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
